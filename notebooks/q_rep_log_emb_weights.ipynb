{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ede13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils functions\n",
    "from utils import load_model, get_final_representation\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Load model\n",
    "MODEL_PATH = \"/home/chashi/Desktop/Research/My Projects/models/Llama-3.1-8B-Instruct\"\n",
    "model, tokenizer = load_model(MODEL_PATH)\n",
    "\n",
    "# Define factual and unfactual questions\n",
    "factual_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How many days are in a year?\",\n",
    "    \"What is 2 plus 2?\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "    \"What is the chemical symbol for water?\",\n",
    "    \"What planet is closest to the Sun?\",\n",
    "    \"How many continents are there?\",\n",
    "    \"What year did World War II end?\",\n",
    "    \"What is the largest ocean on Earth?\",\n",
    "    \"Who painted the Mona Lisa?\"\n",
    "]\n",
    "\n",
    "unfactual_questions = [\n",
    "    \"What color is the sound of Tuesday?\",\n",
    "    \"How many dreams fit in a teaspoon?\",\n",
    "    \"What is the weight of my grandmother's favorite memory?\",\n",
    "    \"Which number tastes the most like purple?\",\n",
    "    \"What will I be thinking about on March 15, 2087?\",\n",
    "    \"How fast do unicorns run?\",\n",
    "    \"What is the temperature of invisible fire?\",\n",
    "    \"Which emotion is exactly 7 inches tall?\",\n",
    "    \"What is the secret ingredient in moonlight?\",\n",
    "    \"How many wishes live in a broken clock?\"\n",
    "]\n",
    "\n",
    "# Define roles\n",
    "roles = [\n",
    "    \"You are a mathematics professor.\",\n",
    "    \"You are a high school student.\", \n",
    "    \"You are a professional chef.\",\n",
    "    \"You are a famous film star.\"\n",
    "]\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "\n",
    "# Create all question-role combinations\n",
    "all_prompts = []\n",
    "labels = []\n",
    "\n",
    "# Base questions without roles (factual)\n",
    "for question in factual_questions:\n",
    "    all_prompts.append(question)\n",
    "    labels.append(\"factual_base\")\n",
    "\n",
    "# Base questions without roles (unfactual)  \n",
    "for question in unfactual_questions:\n",
    "    all_prompts.append(question)\n",
    "    labels.append(\"unfactual_base\")\n",
    "\n",
    "# Factual questions with roles\n",
    "for question in factual_questions:\n",
    "    for role in roles:\n",
    "        prompt = f\"{role} {question}\"\n",
    "        all_prompts.append(prompt)\n",
    "        labels.append(f\"factual_{roles.index(role)}\")\n",
    "\n",
    "# Unfactual questions with roles  \n",
    "for question in unfactual_questions:\n",
    "    for role in roles:\n",
    "        prompt = f\"{role} {question}\"\n",
    "        all_prompts.append(prompt)\n",
    "        labels.append(f\"unfactual_{roles.index(role)}\")\n",
    "\n",
    "print(f\"Total prompts created: {len(all_prompts)}\")\n",
    "\n",
    "# Get LM head weights once\n",
    "lm_head_weights = model.lm_head.weight.detach().cpu().float().numpy()\n",
    "\n",
    "# Extract representations, logits, and LM head embeddings\n",
    "representations = []\n",
    "all_logits = []\n",
    "all_lm_head_embeddings = []\n",
    "\n",
    "for i, prompt in enumerate(all_prompts):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing {i+1}/{len(all_prompts)}\")\n",
    "    \n",
    "    # Get final representation\n",
    "    repr_vec = get_final_representation(model, tokenizer, prompt)\n",
    "    representations.append(repr_vec.numpy())\n",
    "    \n",
    "    # Get logits and LM head embedding for this representation\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Get logits for the last token\n",
    "        last_logits = outputs.logits[0, -1, :].cpu().float().numpy()\n",
    "        all_logits.append(last_logits)\n",
    "        \n",
    "        # Get the nearest LM head embedding\n",
    "        hidden_rep = repr_vec.numpy()\n",
    "        similarities = np.dot(hidden_rep, lm_head_weights.T) / (\n",
    "            np.linalg.norm(hidden_rep) * np.linalg.norm(lm_head_weights, axis=1)\n",
    "        )\n",
    "        nearest_idx = np.argmax(similarities)\n",
    "        all_lm_head_embeddings.append(lm_head_weights[nearest_idx])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "representations = np.array(representations)\n",
    "all_logits = np.array(all_logits)\n",
    "all_lm_head_embeddings = np.array(all_lm_head_embeddings)\n",
    "\n",
    "print(f\"Representations shape: {representations.shape}\")\n",
    "print(f\"Logits shape: {all_logits.shape}\")\n",
    "print(f\"LM head embeddings shape: {all_lm_head_embeddings.shape}\")\n",
    "\n",
    "# Get random samples from embedding and unembedding weights\n",
    "print(\"Sampling embedding and unembedding weights...\")\n",
    "\n",
    "# Embedding weights (input embeddings)\n",
    "embed_weights = model.model.embed_tokens.weight.detach().cpu().float().numpy()\n",
    "vocab_size = embed_weights.shape[0]\n",
    "\n",
    "# Unembedding weights (LM head)\n",
    "unembed_weights = model.lm_head.weight.detach().cpu().float().numpy()\n",
    "\n",
    "# Random sample indices\n",
    "random_indices = random.sample(range(vocab_size), 10000)\n",
    "\n",
    "embed_sample = embed_weights[random_indices]\n",
    "unembed_sample = unembed_weights[random_indices]\n",
    "\n",
    "print(f\"Embedding sample shape: {embed_sample.shape}\")\n",
    "print(f\"Unembedding sample shape: {unembed_sample.shape}\")\n",
    "\n",
    "# Save everything\n",
    "print(\"Saving files...\")\n",
    "\n",
    "# Save all arrays\n",
    "np.save('question_representations.npy', representations)\n",
    "np.save('question_logits.npy', all_logits)\n",
    "np.save('question_lm_head_embeddings.npy', all_lm_head_embeddings)\n",
    "np.save('embedding_weights_sample.npy', embed_sample)\n",
    "np.save('unembedding_weights_sample.npy', unembed_sample)\n",
    "\n",
    "# Save questions and labels as text\n",
    "with open('questions_and_prompts.txt', 'w') as f:\n",
    "    f.write(\"FACTUAL QUESTIONS:\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "    for q in factual_questions:\n",
    "        f.write(f\"{q}\\n\")\n",
    "    \n",
    "    f.write(\"\\nUNFACTUAL QUESTIONS:\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "    for q in unfactual_questions:\n",
    "        f.write(f\"{q}\\n\")\n",
    "    \n",
    "    f.write(\"\\nROLES:\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "    for i, role in enumerate(roles):\n",
    "        f.write(f\"{i}: {role}\\n\")\n",
    "    \n",
    "    f.write(\"\\nALL PROMPTS AND LABELS:\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "    for prompt, label in zip(all_prompts, labels):\n",
    "        f.write(f\"{label}: {prompt}\\n\")\n",
    "\n",
    "# Save labels separately for easy loading\n",
    "with open('labels.txt', 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "print(\"Files saved:\")\n",
    "print(\"- question_representations.npy\")\n",
    "print(\"- question_logits.npy\") \n",
    "print(\"- question_lm_head_embeddings.npy\")\n",
    "print(\"- embedding_weights_sample.npy\") \n",
    "print(\"- unembedding_weights_sample.npy\")\n",
    "print(\"- questions_and_prompts.txt\")\n",
    "print(\"- labels.txt\")\n",
    "\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"- {len(factual_questions)} factual questions\")\n",
    "print(f\"- {len(unfactual_questions)} unfactual questions\") \n",
    "print(f\"- {len(roles)} roles + base (no role)\")\n",
    "print(f\"- {len(all_prompts)} total prompt combinations\")\n",
    "print(f\"- Representation dimension: {representations.shape[1]}\")\n",
    "print(f\"- Logits dimension: {all_logits.shape[1]}\")\n",
    "print(f\"- LM head embeddings dimension: {all_lm_head_embeddings.shape[1]}\")\n",
    "\n",
    "# Quick load test\n",
    "print(\"\\nTesting file loading...\")\n",
    "test_reps = np.load('question_representations.npy')\n",
    "test_logits = np.load('question_logits.npy')\n",
    "test_lm_embeddings = np.load('question_lm_head_embeddings.npy')\n",
    "test_embed = np.load('embedding_weights_sample.npy')\n",
    "test_unembed = np.load('unembedding_weights_sample.npy')\n",
    "\n",
    "print(f\"Loaded representations shape: {test_reps.shape}\")\n",
    "print(f\"Loaded logits shape: {test_logits.shape}\")\n",
    "print(f\"Loaded LM head embeddings shape: {test_lm_embeddings.shape}\")\n",
    "print(f\"Loaded embedding sample shape: {test_embed.shape}\")\n",
    "print(f\"Loaded unembedding sample shape: {test_unembed.shape}\")\n",
    "print(\"All files loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f38ba",
   "metadata": {},
   "source": [
    "# Representation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03304f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# representations: [num_prompts, hidden_dim]\n",
    "print(\"Shape:\", representations.shape)\n",
    "\n",
    "# Norm of each vector (magnitude)\n",
    "norms = np.linalg.norm(representations, axis=1)\n",
    "\n",
    "print(\"=== Norm statistics ===\")\n",
    "print(f\"Min norm: {norms.min():.4f}\")\n",
    "print(f\"Max norm: {norms.max():.4f}\")\n",
    "print(f\"Mean norm: {norms.mean():.4f}\")\n",
    "print(f\"Std norm: {norms.std():.4f}\")\n",
    "\n",
    "# Mean and std per hidden dimension across prompts\n",
    "mean_per_dim = representations.mean(axis=0)\n",
    "std_per_dim = representations.std(axis=0)\n",
    "\n",
    "print(\"\\n=== Per-dimension statistics ===\")\n",
    "print(f\"Mean vector (first 5 dims): {mean_per_dim[:5]}\")\n",
    "print(f\"Std vector (first 5 dims): {std_per_dim[:5]}\")\n",
    "\n",
    "# Optional: pairwise cosine similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# If dataset is small enough, full pairwise\n",
    "cos_sim = cosine_similarity(representations)  # [num_prompts x num_prompts]\n",
    "print(\"\\n=== Pairwise cosine similarity statistics ===\")\n",
    "print(f\"Min: {cos_sim.min():.4f}\")\n",
    "print(f\"Max: {cos_sim.max():.4f}\")\n",
    "print(f\"Mean: {cos_sim.mean():.4f}\")\n",
    "print(f\"Std: {cos_sim.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2edc7f",
   "metadata": {},
   "source": [
    "# Representation Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4beb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Flatten all values\n",
    "all_vals = representations.flatten()\n",
    "\n",
    "plt.hist(all_vals, bins=100, density=True)\n",
    "plt.title(\"Distribution of all representation values\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Min: {all_vals.min():.4f}, Max: {all_vals.max():.4f}\")\n",
    "print(f\"Mean: {all_vals.mean():.4f}, Std: {all_vals.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4b04f",
   "metadata": {},
   "source": [
    "# Unembedding weight stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b058ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Move unembedding weights to GPU\n",
    "W = model.lm_head.weight.detach().float().cuda()  # [vocab_size, hidden_dim]\n",
    "vocab_size, hidden_dim = W.shape\n",
    "\n",
    "# Norm statistics\n",
    "norms = torch.norm(W, dim=1)\n",
    "print(\"=== Norm statistics ===\")\n",
    "print(f\"Min norm: {norms.min().item():.4f}\")\n",
    "print(f\"Max norm: {norms.max().item():.4f}\")\n",
    "print(f\"Mean norm: {norms.mean().item():.4f}\")\n",
    "print(f\"Std norm: {norms.std().item():.4f}\")\n",
    "\n",
    "# Pairwise distance stats (chunked to avoid OOM)\n",
    "batch = 10000\n",
    "stats = {\"min\": float(\"inf\"), \"max\": -float(\"inf\"), \"sum\": 0.0, \"sumsq\": 0.0, \"count\": 0}\n",
    "\n",
    "for i in range(0, vocab_size, batch):\n",
    "    x = W[i:i+batch]  # [batch, hidden_dim]\n",
    "    # Compute distances to all vectors\n",
    "    dists = torch.cdist(x, W)  # [batch, vocab_size]\n",
    "    \n",
    "    stats[\"min\"] = min(stats[\"min\"], dists.min().item())\n",
    "    stats[\"max\"] = max(stats[\"max\"], dists.max().item())\n",
    "    stats[\"sum\"] += dists.sum().item()\n",
    "    stats[\"sumsq\"] += (dists ** 2).sum().item()\n",
    "    stats[\"count\"] += dists.numel()\n",
    "\n",
    "mean = stats[\"sum\"] / stats[\"count\"]\n",
    "var = stats[\"sumsq\"] / stats[\"count\"] - mean**2\n",
    "std = var**0.5\n",
    "\n",
    "print(\"\\n=== Pairwise Euclidean Distance Statistics ===\")\n",
    "print(f\"Min distance: {stats['min']:.4f}\")\n",
    "print(f\"Max distance: {stats['max']:.4f}\")\n",
    "print(f\"Mean distance: {mean:.4f}\")\n",
    "print(f\"Std distance: {std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881ed02",
   "metadata": {},
   "source": [
    "# Weights tied or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if embedding and unembedding weights are tied\n",
    "def check_weight_tying(model):\n",
    "    # Get embedding layer weights\n",
    "    embed_weights = model.model.embed_tokens.weight\n",
    "    \n",
    "    # Get LM head weights  \n",
    "    lm_head_weights = model.lm_head.weight\n",
    "    \n",
    "    # Check if they share the same memory (tied)\n",
    "    is_same_object = embed_weights is lm_head_weights\n",
    "    \n",
    "    # Check if they have the same values (even if different objects)\n",
    "    if not is_same_object:\n",
    "        is_same_values = torch.equal(embed_weights, lm_head_weights)\n",
    "    else:\n",
    "        is_same_values = True\n",
    "    \n",
    "    print(f\"Embedding shape: {embed_weights.shape}\")\n",
    "    print(f\"LM head shape: {lm_head_weights.shape}\")\n",
    "    print(f\"Same memory object: {is_same_object}\")\n",
    "    print(f\"Same values: {is_same_values}\")\n",
    "    print(f\"Weight tying: {'TIED' if is_same_object else 'UNTIED'}\")\n",
    "    \n",
    "    return is_same_object\n",
    "\n",
    "# Run the check\n",
    "check_weight_tying(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f18a01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd760f71",
   "metadata": {},
   "source": [
    "# Embedding vs unembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2216e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils functions\n",
    "from utils import load_model\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "\n",
    "# Load model\n",
    "MODEL_PATH = \"/home/chashi/Desktop/Research/My Projects/models/Llama-3.1-8B-Instruct\"\n",
    "model, tokenizer = load_model(MODEL_PATH)\n",
    "\n",
    "def check_weight_tying(model):\n",
    "    \"\"\"Check if embedding and unembedding weights are tied\"\"\"\n",
    "    embed_weights = model.model.embed_tokens.weight\n",
    "    lm_head_weights = model.lm_head.weight\n",
    "    \n",
    "    is_same_object = embed_weights is lm_head_weights\n",
    "    \n",
    "    if not is_same_object:\n",
    "        is_same_values = torch.equal(embed_weights, lm_head_weights)\n",
    "    else:\n",
    "        is_same_values = True\n",
    "    \n",
    "    print(f\"Embedding shape: {embed_weights.shape}\")\n",
    "    print(f\"LM head shape: {lm_head_weights.shape}\")\n",
    "    print(f\"Same memory object: {is_same_object}\")\n",
    "    print(f\"Same values: {is_same_values}\")\n",
    "    print(f\"Weight tying: {'TIED' if is_same_object else 'UNTIED'}\")\n",
    "    \n",
    "    return is_same_object\n",
    "\n",
    "def stratified_vocab_sample(vocab_size, sample_size=3000):\n",
    "    \"\"\"Sample tokens from different frequency ranges\"\"\"\n",
    "    # Assume frequency decreases with token index\n",
    "    high_freq_size = min(1000, sample_size // 3)\n",
    "    mid_freq_size = min(sample_size // 3, 2000)\n",
    "    low_freq_size = sample_size - high_freq_size - mid_freq_size\n",
    "    \n",
    "    # High frequency tokens (first 1000)\n",
    "    high_freq = np.arange(0, min(1000, vocab_size))\n",
    "    \n",
    "    # Mid frequency tokens  \n",
    "    mid_start = 1000\n",
    "    mid_end = min(10000, vocab_size)\n",
    "    if mid_end > mid_start:\n",
    "        mid_freq = np.random.choice(np.arange(mid_start, mid_end), \n",
    "                                   min(mid_freq_size, mid_end - mid_start), \n",
    "                                   replace=False)\n",
    "    else:\n",
    "        mid_freq = np.array([])\n",
    "    \n",
    "    # Low frequency tokens\n",
    "    low_start = 10000\n",
    "    if low_start < vocab_size:\n",
    "        low_freq = np.random.choice(np.arange(low_start, vocab_size), \n",
    "                                   min(low_freq_size, vocab_size - low_start), \n",
    "                                   replace=False)\n",
    "    else:\n",
    "        low_freq = np.array([])\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    all_indices = np.concatenate([high_freq[:high_freq_size], mid_freq, low_freq])\n",
    "    np.random.shuffle(all_indices)\n",
    "    \n",
    "    return all_indices[:sample_size]\n",
    "\n",
    "def analyze_weight_space_distances(model, sample_size=3000):\n",
    "    \"\"\"Analyze distance distributions in embedding and unembedding spaces\"\"\"\n",
    "    \n",
    "    print(\"=== WEIGHT SPACE DISTANCE ANALYSIS ===\")\n",
    "    \n",
    "    # Get the weight matrices\n",
    "    embed_weights = model.model.embed_tokens.weight.detach().cpu().float().numpy()\n",
    "    lm_head_weights = model.lm_head.weight.detach().cpu().float().numpy()\n",
    "    \n",
    "    vocab_size = embed_weights.shape[0]\n",
    "    hidden_size = embed_weights.shape[1]\n",
    "    \n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"Hidden size: {hidden_size}\")\n",
    "    print(f\"Using sample size: {sample_size} ({sample_size/vocab_size*100:.2f}% of vocabulary)\")\n",
    "    \n",
    "    # Stratified sampling\n",
    "    sample_indices = stratified_vocab_sample(vocab_size, sample_size)\n",
    "    \n",
    "    # Sample the weight matrices\n",
    "    embed_sample = embed_weights[sample_indices]\n",
    "    lm_head_sample = lm_head_weights[sample_indices]\n",
    "    \n",
    "    print(f\"Sampled embedding shape: {embed_sample.shape}\")\n",
    "    print(f\"Sampled LM head shape: {lm_head_sample.shape}\")\n",
    "    \n",
    "    # Calculate pairwise distances\n",
    "    print(\"Calculating pairwise distances...\")\n",
    "    \n",
    "    # Embedding space distances\n",
    "    embed_cosine_dist = cosine_distances(embed_sample)\n",
    "    embed_euclidean_dist = euclidean_distances(embed_sample)\n",
    "    \n",
    "    # Unembedding space distances  \n",
    "    lm_head_cosine_dist = cosine_distances(lm_head_sample)\n",
    "    lm_head_euclidean_dist = euclidean_distances(lm_head_sample)\n",
    "    \n",
    "    # Extract upper triangle (avoid diagonal and duplicates)\n",
    "    mask = np.triu(np.ones_like(embed_cosine_dist, dtype=bool), k=1)\n",
    "    \n",
    "    embed_cosine_flat = embed_cosine_dist[mask]\n",
    "    embed_euclidean_flat = embed_euclidean_dist[mask]\n",
    "    lm_head_cosine_flat = lm_head_cosine_dist[mask]\n",
    "    lm_head_euclidean_flat = lm_head_euclidean_dist[mask]\n",
    "    \n",
    "    print(f\"Number of distance pairs analyzed: {len(embed_cosine_flat):,}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n=== DISTANCE STATISTICS ===\")\n",
    "    print(\"EMBEDDING SPACE:\")\n",
    "    print(f\"  Cosine distance - Min: {embed_cosine_flat.min():.6f}, Max: {embed_cosine_flat.max():.6f}\")\n",
    "    print(f\"  Cosine distance - Mean: {embed_cosine_flat.mean():.6f}, Std: {embed_cosine_flat.std():.6f}\")\n",
    "    print(f\"  Cosine distance - Median: {np.median(embed_cosine_flat):.6f}\")\n",
    "    print(f\"  Euclidean distance - Min: {embed_euclidean_flat.min():.6f}, Max: {embed_euclidean_flat.max():.6f}\")\n",
    "    print(f\"  Euclidean distance - Mean: {embed_euclidean_flat.mean():.6f}, Std: {embed_euclidean_flat.std():.6f}\")\n",
    "    print(f\"  Euclidean distance - Median: {np.median(embed_euclidean_flat):.6f}\")\n",
    "    \n",
    "    print(\"\\nUNEMBEDDING SPACE:\")\n",
    "    print(f\"  Cosine distance - Min: {lm_head_cosine_flat.min():.6f}, Max: {lm_head_cosine_flat.max():.6f}\")\n",
    "    print(f\"  Cosine distance - Mean: {lm_head_cosine_flat.mean():.6f}, Std: {lm_head_cosine_flat.std():.6f}\")\n",
    "    print(f\"  Cosine distance - Median: {np.median(lm_head_cosine_flat):.6f}\")\n",
    "    print(f\"  Euclidean distance - Min: {lm_head_euclidean_flat.min():.6f}, Max: {lm_head_euclidean_flat.max():.6f}\")\n",
    "    print(f\"  Euclidean distance - Mean: {lm_head_euclidean_flat.mean():.6f}, Std: {lm_head_euclidean_flat.std():.6f}\")\n",
    "    print(f\"  Euclidean distance - Median: {np.median(lm_head_euclidean_flat):.6f}\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\n=== SPACE COMPARISON ===\")\n",
    "    print(f\"Cosine distance difference (LM_head - Embedding):\")\n",
    "    print(f\"  Mean difference: {lm_head_cosine_flat.mean() - embed_cosine_flat.mean():.6f}\")\n",
    "    print(f\"  Std difference: {lm_head_cosine_flat.std() - embed_cosine_flat.std():.6f}\")\n",
    "    print(f\"Euclidean distance difference (LM_head - Embedding):\")\n",
    "    print(f\"  Mean difference: {lm_head_euclidean_flat.mean() - embed_euclidean_flat.mean():.6f}\")\n",
    "    print(f\"  Std difference: {lm_head_euclidean_flat.std() - embed_euclidean_flat.std():.6f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Cosine distance histograms\n",
    "    axes[0,0].hist(embed_cosine_flat, bins=50, alpha=0.7, label='Embedding', density=True, color='blue')\n",
    "    axes[0,0].hist(lm_head_cosine_flat, bins=50, alpha=0.7, label='LM Head', density=True, color='red')\n",
    "    axes[0,0].set_title('Cosine Distance Distributions')\n",
    "    axes[0,0].set_xlabel('Distance')\n",
    "    axes[0,0].set_ylabel('Density')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Euclidean distance histograms\n",
    "    axes[0,1].hist(embed_euclidean_flat, bins=50, alpha=0.7, label='Embedding', density=True, color='blue')\n",
    "    axes[0,1].hist(lm_head_euclidean_flat, bins=50, alpha=0.7, label='LM Head', density=True, color='red')\n",
    "    axes[0,1].set_title('Euclidean Distance Distributions')\n",
    "    axes[0,1].set_xlabel('Distance')\n",
    "    axes[0,1].set_ylabel('Density')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plots for cosine\n",
    "    axes[0,2].boxplot([embed_cosine_flat, lm_head_cosine_flat], \n",
    "                      labels=['Embedding', 'LM Head'])\n",
    "    axes[0,2].set_title('Cosine Distance Box Plots')\n",
    "    axes[0,2].set_ylabel('Distance')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plots for euclidean\n",
    "    axes[1,0].boxplot([embed_euclidean_flat, lm_head_euclidean_flat],\n",
    "                      labels=['Embedding', 'LM Head'])\n",
    "    axes[1,0].set_title('Euclidean Distance Box Plots')\n",
    "    axes[1,0].set_ylabel('Distance')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot comparison\n",
    "    sample_indices_plot = np.random.choice(len(embed_cosine_flat), 5000, replace=False)\n",
    "    axes[1,1].scatter(embed_cosine_flat[sample_indices_plot], \n",
    "                      lm_head_cosine_flat[sample_indices_plot], \n",
    "                      alpha=0.5, s=1)\n",
    "    axes[1,1].set_xlabel('Embedding Cosine Distance')\n",
    "    axes[1,1].set_ylabel('LM Head Cosine Distance')\n",
    "    axes[1,1].set_title('Cosine Distance Correlation')\n",
    "    axes[1,1].plot([0, 2], [0, 2], 'r--', alpha=0.8)  # y=x line\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Difference histogram\n",
    "    cosine_diff = lm_head_cosine_flat - embed_cosine_flat\n",
    "    axes[1,2].hist(cosine_diff, bins=50, alpha=0.7, color='green')\n",
    "    axes[1,2].set_title('Cosine Distance Difference\\n(LM Head - Embedding)')\n",
    "    axes[1,2].set_xlabel('Distance Difference')\n",
    "    axes[1,2].set_ylabel('Frequency')\n",
    "    axes[1,2].axvline(0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    cosine_corr = np.corrcoef(embed_cosine_flat, lm_head_cosine_flat)[0,1]\n",
    "    euclidean_corr = np.corrcoef(embed_euclidean_flat, lm_head_euclidean_flat)[0,1]\n",
    "    \n",
    "    print(f\"\\n=== CORRELATION ANALYSIS ===\")\n",
    "    print(f\"Cosine distance correlation: {cosine_corr:.6f}\")\n",
    "    print(f\"Euclidean distance correlation: {euclidean_corr:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'embed_cosine': embed_cosine_flat,\n",
    "        'embed_euclidean': embed_euclidean_flat,\n",
    "        'lm_head_cosine': lm_head_cosine_flat,\n",
    "        'lm_head_euclidean': lm_head_euclidean_flat,\n",
    "        'sample_indices': sample_indices,\n",
    "        'correlations': {'cosine': cosine_corr, 'euclidean': euclidean_corr}\n",
    "    }\n",
    "\n",
    "# Run the complete analysis\n",
    "print(\"Checking weight tying...\")\n",
    "is_tied = check_weight_tying(model)\n",
    "\n",
    "print(f\"\\nAnalyzing weight space distances...\")\n",
    "distance_results = analyze_weight_space_distances(model, sample_size=3000)\n",
    "\n",
    "print(f\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cae010",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
