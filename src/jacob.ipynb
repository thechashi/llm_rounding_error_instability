{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "\n",
    "last_token_embedding = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize model and data\n",
    "# Load the pretrained model and tokenizer\n",
    "def initialize_model_and_tokenizer(model_name):\n",
    "    # Example: HuggingFace Transformers\n",
    "    if model_name == \"gpt2\":\n",
    "        from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    elif model_name == \"distilgpt2\":\n",
    "        from transformers import DistilGPT2LMHeadModel, DistilGPT2Tokenizer\n",
    "        model = DistilGPT2LMHeadModel.from_pretrained(model_name)\n",
    "        tokenizer = DistilGPT2Tokenizer.from_pretrained(model_name)\n",
    "    elif model_name == \"EleutherAI/gpt-neo-125M\":\n",
    "        from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "        model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    elif model_name == \"t5-small\":\n",
    "        from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    elif model_name == \"bert-base-uncased\":\n",
    "        from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "        model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    elif model_name == \"albert-base-v2\":\n",
    "        from transformers import AlbertForQuestionAnswering, AlbertTokenizer\n",
    "        model = AlbertForQuestionAnswering.from_pretrained(model_name)\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "    else:\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute the Jacobian\n",
    "def model_forward(model, flattened_embeddings, original_shape):\n",
    "    \"\"\"\n",
    "    Forward pass of the model.\n",
    "    \"\"\"\n",
    "    # Restore the original shape\n",
    "    input_embeddings = flattened_embeddings.view(original_shape)\n",
    "\n",
    "    output = model(inputs_embeds=input_embeddings, output_hidden_states=True)\n",
    "    last_hidden_state = output.hidden_states[-1][:,-1,:]  # Choose the last hidden state of the last layer\n",
    "    global last_token_embeddings\n",
    "    last_token_embeddings = last_hidden_state.squeeze(0)\n",
    "    return last_token_embeddings\n",
    "\n",
    "def compute_jacobian(model, input_embeddings, target_layer):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian of the model's output w.r.t input embeddings.\n",
    "    \"\"\"\n",
    "    original_shape = input_embeddings.shape\n",
    "\n",
    "    flattened_embeddings = input_embeddings.flatten()\n",
    "    flattened_embeddings.requires_grad_(True)\n",
    "\n",
    "    from functools import partial\n",
    "\n",
    "    # Freeze the `model` argument\n",
    "    model_forward_partial = partial(model_forward, model,original_shape=original_shape)\n",
    "\n",
    "    jacobian = torch.autograd.functional.jacobian(\n",
    "        model_forward_partial,\n",
    "        flattened_embeddings, \n",
    "        vectorize=True)\n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Perform SVD on the Jacobian\n",
    "def perform_svd(jacobian):\n",
    "    \"\"\"\n",
    "    Perform Singular Value Decomposition on the Jacobian.\n",
    "    \"\"\"\n",
    "    U, S, Vt = torch.linalg.svd(jacobian, full_matrices=False)\n",
    "    print(\"U shape:\", U.shape)\n",
    "    print(\"S shape:\", S.shape)\n",
    "    print(\"Vt shape:\", Vt.shape)\n",
    "    return U, S, Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute the difference embedding\n",
    "def compute_difference_embedding(target_embedding, last_token_embedding):\n",
    "    \"\"\"\n",
    "    Compute the normalized difference between embeddings.\n",
    "    \"\"\"\n",
    "    diff = target_embedding - last_token_embedding\n",
    "    print(diff.shape)\n",
    "    return diff / torch.norm(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Project difference embedding onto top-k directions\n",
    "def project_onto_top_k(U, diff_embedding, k):\n",
    "    \"\"\"\n",
    "    Project the difference embedding onto the top-k singular directions.\n",
    "    \"\"\"\n",
    "    print(f\"U.Shape: {U.shape}\")\n",
    "    top_k_directions = U[:, :k]\n",
    "    print(top_k_directions.shape)\n",
    "    diff_embedding = diff_embedding.flatten()\n",
    "    print(diff_embedding.shape)\n",
    "    projection = top_k_directions.T @ diff_embedding\n",
    "    print(\"projection.shape:\", projection.shape)\n",
    "    return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Solve least squares for coefficients\n",
    "def solve_least_squares(projection, singular_values):\n",
    "    \"\"\"\n",
    "    Solve for coefficients using least squares.\n",
    "    \"\"\"\n",
    "    scaled_projection = projection / singular_values[:len(projection)]\n",
    "    return scaled_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Iteratively update input embeddings\n",
    "def iterative_update(model,input_embedding, jacobian, target_embedding, target_layer, iterations, step_size, k):\n",
    "    \"\"\"\n",
    "    Iteratively update the input embedding along influential directions.\n",
    "    \"\"\"\n",
    "    for _ in range(iterations):\n",
    "        U, S, Vt = perform_svd(jacobian)\n",
    "        diff_embedding = compute_difference_embedding(target_embedding, last_token_embeddings)\n",
    "        projection = project_onto_top_k(U, diff_embedding, k)\n",
    "        coefficients = solve_least_squares(projection, S)\n",
    "        delta = (U[:, :k] @ coefficients).view_as(input_embedding)\n",
    "        input_embedding = input_embedding + step_size * delta\n",
    "        # Recompute the Jacobian after updating input embeddings\n",
    "        jacobian = compute_jacobian(model, input_embedding, target_layer)\n",
    "    return input_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Main function to run Jacob's Ladder\n",
    "def jacobs_ladder_pipeline(model, tokenizer, input_text, target_text, iterations=10, step_size=0.1, k=5):\n",
    "    \"\"\"\n",
    "    Full pipeline for Jacob's Ladder.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_embeddings = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=True)[\"input_ids\"]\n",
    "    target_embeddings = tokenizer(target_text, return_tensors=\"pt\", add_special_tokens=True)[\"input_ids\"]\n",
    "    \n",
    "    # Convert input IDs to embeddings\n",
    "    input_embeddings = model.get_input_embeddings()(input_embeddings)\n",
    "    print(\"Input embeddings shape:\", input_embeddings.shape)\n",
    "    target_embeddings = model.get_input_embeddings()(target_embeddings)[:,-1,:].unsqueeze(0)\n",
    "    \n",
    "    # Choose the layer for Jacobian computation\n",
    "    target_layer = -1  # Example: Last layer\n",
    "    \n",
    "    # Compute initial Jacobian\n",
    "    jacobian = compute_jacobian(model, input_embeddings, target_layer)\n",
    "    print(\"Jacobian shape:\", jacobian.shape)\n",
    "    return jacobian\n",
    "    \n",
    "    # # Iterative updates\n",
    "    # optimized_embedding = iterative_update(\n",
    "    #     model,input_embeddings, jacobian, target_embeddings, target_layer, iterations, step_size, k\n",
    "    # )\n",
    "    \n",
    "    # return optimized_embedding\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"gpt2\"  # Replace with your model\n",
    "# model_name = \"distilgpt2\"\n",
    "# model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "# model_name = \"albert-base-v2\"\n",
    "model, tokenizer = initialize_model_and_tokenizer(model_name)\n",
    "input_text = \"Hello, how are you?\"\n",
    "target_text = \"Sure, I can help with that.\"\n",
    "# optimized_embedding = jacobs_ladder_pipeline(model, tokenizer, input_text, target_text)\n",
    "jacobian = jacobs_ladder_pipeline(model, tokenizer, input_text, target_text)\n",
    "\n",
    "# print(\"Optimized embedding:\", optimized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completions(directions):\n",
    "    print(directions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD and process the directions\n",
    "U, S, Vt = perform_svd(jacobian)\n",
    "generate_completions(Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_embeddings(model, tokenizer, input_embeddings, max_length=50, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generates text from input embeddings and decodes it into plain language.\n",
    "    \n",
    "    Args:\n",
    "        model: The Hugging Face model instance.\n",
    "        tokenizer: The corresponding tokenizer for the model.\n",
    "        input_embeddings: A tensor of shape [batch_size, seq_len, hidden_dim].\n",
    "        max_length: The maximum length of the generated text.\n",
    "        num_return_sequences: Number of completions to generate for each input.\n",
    "        \n",
    "    Returns:\n",
    "        A list of generated texts.\n",
    "    \"\"\"\n",
    "    # Ensure input_embeddings is on the same device as the model\n",
    "    input_embeddings = input_embeddings.to(model.device)\n",
    "    \n",
    "    # Generate outputs\n",
    "    generated_ids = model.generate(\n",
    "        inputs_embeds=input_embeddings,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,  # Enables sampling for diverse outputs\n",
    "        top_k=50,        # Use top-k sampling\n",
    "        top_p=0.95       # Use nucleus sampling\n",
    "    )\n",
    "    \n",
    "    # Decode the generated token IDs into plain text\n",
    "    decoded_outputs = [tokenizer.decode(output_id, skip_special_tokens=True) for output_id in generated_ids]\n",
    "    \n",
    "    return decoded_outputs\n",
    "\n",
    "def generate_from_text(model, tokenizer, input_texts, max_length=50, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generates text from input text strings, processes the input as token IDs,\n",
    "    and decodes the output into plain language.\n",
    "    \n",
    "    Args:\n",
    "        model: The Hugging Face model instance.\n",
    "        tokenizer: The corresponding tokenizer for the model.\n",
    "        input_texts: A list of input strings.\n",
    "        max_length: The maximum length of the generated text.\n",
    "        num_return_sequences: Number of completions to generate for each input.\n",
    "        \n",
    "    Returns:\n",
    "        A list of generated texts for each input.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text to get input IDs\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    # Generate outputs\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=False,  # Enables sampling for diverse outputs\n",
    "        top_k=50,        # Use top-k sampling\n",
    "        top_p=0.95       # Use nucleus sampling\n",
    "    )\n",
    "    \n",
    "    # Decode the generated token IDs into plain text\n",
    "    decoded_outputs = [tokenizer.decode(output_id, skip_special_tokens=True) for output_id in generated_ids]\n",
    "    \n",
    "    return decoded_outputs\n",
    "\n",
    "generate_from_text(model, tokenizer, [\"Hello, how are you?\", \"He was teaching me the steps to make a bomb and they were:\"])\n",
    "# generate_from_embeddings(model, tokenizer, input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_ids = model.generate(inputs_embeds=input_embeddings, max_length=max_length)\n",
    "\n",
    "# Decode the generated token IDs to get the generated text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jacob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
