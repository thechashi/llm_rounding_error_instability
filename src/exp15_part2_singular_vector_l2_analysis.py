"""
Experiment 15 Part 2: Singular Vector Stability Boundary L2 Distance Analysis

This experiment builds on Experiment 13 by analyzing the effects of the
smallest possible perturbation that causes a change in the model's output
along each singular vector direction.

Purpose:
--------
To quantify the magnitude of the change in both the input embedding and the
final hidden state representation when crossing the stability boundary along
each singular vector direction.

Methodology:
------------
1.  Load the .npz data file generated by exp13_singular_vector_stability.py,
    which contains the singular vector indices and the maximum stable perturbation
    magnitudes (max_s) for each singular vector.
2.  Load the Llama model and tokenizer.
3.  Recompute the Jacobian SVD to obtain the singular vectors (Vt).
4.  Recreate the original input embedding and original final hidden state.
5.  For each singular vector index and its corresponding max_s:
    a.  Calculate `min_t`, the next representable float value after `max_s` in
        the direction of infinity. This is the smallest perturbation that is
        guaranteed to cause a change in the output.
    b.  Construct the perturbed embedding using `min_t` along that singular vector.
    c.  Calculate the L2 distance between the original embedding and the
        perturbed embedding.
    d.  Pass the perturbed embedding through the model to get the new, changed
        final hidden state.
    e.  Calculate the L2 distance between the original final hidden state and
        the new, changed final hidden state.
6.  Aggregate and analyze these L2 distances (e.g., mean, max, min).
7.  Save the results to a new .npz and .csv file.

Output:
-------
- A new NPZ file containing the original data plus the calculated L2 distances.
- A CSV file for easy analysis.
- Plots visualizing the L2 distances vs. the singular vector index.
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import argparse
from tqdm import tqdm

def load_model(model_path="/home/chashi/Desktop/Research/My Projects/models/Llama-3.1-8B-Instruct"):
    """Load model and tokenizer in float32"""
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        torch_dtype=torch.float32
    )
    return model, tokenizer

def compute_jacobian_svd(model, embeddings, last_token_idx):
    """Compute Jacobian and SVD for the model"""
    def forward_fn(flat_emb):
        emb = flat_emb.view(1, -1)
        mod_emb = embeddings.clone()
        mod_emb[0, last_token_idx, :] = emb
        outputs = model(inputs_embeds=mod_emb, output_hidden_states=True)
        return outputs.hidden_states[-1][0, last_token_idx, :]

    last_emb = embeddings[0, last_token_idx, :].clone().detach().requires_grad_(True)
    jacobian = torch.autograd.functional.jacobian(forward_fn, last_emb, vectorize=True)
    U, S, Vt = torch.linalg.svd(jacobian, full_matrices=False)
    return U, S, Vt

def get_hidden_state(model, embeddings, last_token_idx):
    """Get the final hidden state for the last token"""
    with torch.no_grad():
        outputs = model(inputs_embeds=embeddings, output_hidden_states=True)
        hidden_state = outputs.hidden_states[-1][0, last_token_idx, :].cpu().numpy()
    return hidden_state

def analyze_singular_vector_distances(npz_path, output_dir, precision="float64"):
    """
    Main function to perform L2 distance analysis at the stability boundary
    for each singular vector direction.
    """
    print("="*80)
    print("EXPERIMENT 15 PART 2: Singular Vector Stability Boundary L2 Distance Analysis")
    print("="*80)
    print(f"Loading data from: {npz_path}")
    print(f"Using precision for perturbation: {precision}")

    # Load data from exp13
    data = np.load(npz_path, allow_pickle=True)
    singular_vector_indices = data['singular_vector_indices']
    max_s_values = data['max_s_values']
    input_text = data['input_text'].item()
    num_vectors = len(singular_vector_indices)

    print(f"Found {num_vectors} singular vectors to process.")
    print(f"Input text: '{input_text}'")

    # Load model
    print("\n[1/6] Loading model...")
    model, tokenizer = load_model()
    device = next(model.parameters()).device

    # Tokenize input and get original embedding and hidden state
    print("[2/6] Recreating original embedding and hidden state...")
    inputs = tokenizer(input_text, return_tensors="pt").to(device)
    with torch.no_grad():
        original_embeddings = model.model.embed_tokens(inputs["input_ids"])

    last_idx = inputs["input_ids"].shape[1] - 1
    original_hidden_state = get_hidden_state(model, original_embeddings, last_idx)
    original_emb_token = original_embeddings[0, last_idx, :].clone()

    # Recompute SVD to get singular vectors
    print("[3/6] Recomputing Jacobian SVD to obtain singular vectors...")
    U, S, Vt = compute_jacobian_svd(model, original_embeddings, last_idx)

    print(f"\nTop 5 singular values:")
    for i in range(min(5, len(S))):
        print(f"  Ïƒ_{i} = {S[i].item():.6f}")

    embedding_l2_distances = np.zeros(num_vectors)
    hidden_state_l2_distances = np.zeros(num_vectors)

    print("\n[4/6] Calculating L2 distances at the boundary...")
    for i in tqdm(range(num_vectors), desc="Processing singular vectors"):
        max_s = np.float32(max_s_values[i])
        min_t = np.nextafter(max_s, np.float32(np.inf))

        # Get the i-th singular vector as the perturbation direction
        direction = Vt[i, :].to(device).float()

        # Perturb embedding with min_t
        if precision == "float64":
            perturbed_emb_t = (original_emb_token.double() + min_t * direction.double()).float()
        else: # float32
            perturbed_emb_t = original_emb_token + torch.tensor(min_t, device=device) * direction

        # Calculate L2 distance for the embedding
        embedding_l2_distances[i] = torch.norm(original_emb_token - perturbed_emb_t).item()

        # Get the new hidden state
        perturbed_embeddings = original_embeddings.clone()
        perturbed_embeddings[0, last_idx, :] = perturbed_emb_t
        changed_hidden_state = get_hidden_state(model, perturbed_embeddings, last_idx)

        # Calculate L2 distance for the hidden state
        hidden_state_l2_distances[i] = np.linalg.norm(original_hidden_state - changed_hidden_state)

    print("\n[5/6] Saving results...")
    os.makedirs(output_dir, exist_ok=True)

    output_npz_path = os.path.join(output_dir, "singular_vector_distance_data.npz")
    np.savez(
        output_npz_path,
        singular_vector_indices=singular_vector_indices,
        max_s_values=max_s_values,
        embedding_l2_distances=embedding_l2_distances,
        hidden_state_l2_distances=hidden_state_l2_distances,
        singular_values_all=S.cpu().numpy(),
        input_text=input_text,
    )
    print(f"Saved data to NPZ: {output_npz_path}")

    csv_path = os.path.join(output_dir, "singular_vector_distance_data.csv")
    csv_data = np.vstack((singular_vector_indices, max_s_values, embedding_l2_distances, hidden_state_l2_distances)).T
    np.savetxt(csv_path, csv_data, delimiter=",",
               header="singular_vector_index,max_s,embedding_l2_dist,hidden_state_l2_dist", comments="")
    print(f"Saved data to CSV: {csv_path}")

    print("\n" + "="*80)
    print("DISTANCE STATISTICS")
    print("="*80)
    print("--- Embedding L2 Distance ---")
    print(f"Mean:   {np.mean(embedding_l2_distances):.6e}")
    print(f"Median: {np.median(embedding_l2_distances):.6e}")
    print(f"Min:    {np.min(embedding_l2_distances):.6e}")
    print(f"Max:    {np.max(embedding_l2_distances):.6e}")
    print(f"Std:    {np.std(embedding_l2_distances):.6e}")
    print("\n--- Hidden State L2 Distance ---")
    print(f"Mean:   {np.mean(hidden_state_l2_distances):.6e}")
    print(f"Median: {np.median(hidden_state_l2_distances):.6e}")
    print(f"Min:    {np.min(hidden_state_l2_distances):.6e}")
    print(f"Max:    {np.max(hidden_state_l2_distances):.6e}")
    print(f"Std:    {np.std(hidden_state_l2_distances):.6e}")
    print("="*80)

    print("\n[6/6] Creating plots...")

    # Plotting embedding distances
    fig, ax = plt.subplots(figsize=(16, 8))
    ax.plot(singular_vector_indices, embedding_l2_distances, color='purple', linewidth=1, alpha=0.7)
    ax.axhline(y=np.mean(embedding_l2_distances), color='red', linestyle='--',
               label=f'Mean: {np.mean(embedding_l2_distances):.2e}')
    ax.axhline(y=np.median(embedding_l2_distances), color='green', linestyle=':',
               label=f'Median: {np.median(embedding_l2_distances):.2e}')
    ax.set_title(f'Embedding L2 Distance at Stability Boundary (Singular Vectors)\n"{input_text}"',
                 fontsize=14, fontweight='bold')
    ax.set_xlabel('Singular Vector Index')
    ax.set_ylabel('L2 Distance (Embedding vs Perturbed)')
    ax.grid(True, alpha=0.5)
    ax.legend()
    plt.tight_layout()
    emb_dist_path = os.path.join(output_dir, "embedding_l2_distance.png")
    plt.savefig(emb_dist_path, dpi=300)
    print(f"Saved embedding distance plot to: {emb_dist_path}")
    plt.close()

    # Plotting hidden state distances
    fig, ax = plt.subplots(figsize=(16, 8))
    ax.plot(singular_vector_indices, hidden_state_l2_distances, color='green', linewidth=1, alpha=0.7)
    ax.axhline(y=np.mean(hidden_state_l2_distances), color='red', linestyle='--',
               label=f'Mean: {np.mean(hidden_state_l2_distances):.2e}')
    ax.axhline(y=np.median(hidden_state_l2_distances), color='orange', linestyle=':',
               label=f'Median: {np.median(hidden_state_l2_distances):.2e}')
    ax.set_title(f'Hidden State L2 Distance at Stability Boundary (Singular Vectors)\n"{input_text}"',
                 fontsize=14, fontweight='bold')
    ax.set_xlabel('Singular Vector Index')
    ax.set_ylabel('L2 Distance (Original vs Changed Hidden State)')
    ax.grid(True, alpha=0.5)
    ax.legend()
    plt.tight_layout()
    hidden_dist_path = os.path.join(output_dir, "hidden_state_l2_distance.png")
    plt.savefig(hidden_dist_path, dpi=300)
    print(f"Saved hidden state distance plot to: {hidden_dist_path}")
    plt.close()

    # Combined plot with dual y-axes
    fig, ax1 = plt.subplots(figsize=(16, 8))

    color1 = 'purple'
    ax1.set_xlabel('Singular Vector Index', fontsize=12)
    ax1.set_ylabel('Embedding L2 Distance', color=color1, fontsize=12)
    ax1.plot(singular_vector_indices, embedding_l2_distances, color=color1, linewidth=1, alpha=0.7, label='Embedding L2')
    ax1.tick_params(axis='y', labelcolor=color1)
    ax1.grid(True, alpha=0.3)

    ax2 = ax1.twinx()
    color2 = 'green'
    ax2.set_ylabel('Hidden State L2 Distance', color=color2, fontsize=12)
    ax2.plot(singular_vector_indices, hidden_state_l2_distances, color=color2, linewidth=1, alpha=0.7, label='Hidden State L2')
    ax2.tick_params(axis='y', labelcolor=color2)

    plt.title(f'L2 Distances at Stability Boundary (Singular Vectors)\n"{input_text}"',
              fontsize=14, fontweight='bold', pad=20)

    # Combine legends
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')

    plt.tight_layout()
    combined_path = os.path.join(output_dir, "combined_l2_distances.png")
    plt.savefig(combined_path, dpi=300)
    print(f"Saved combined plot to: {combined_path}")
    plt.close()

    # Create log-scale plot for embedding distances if there's high variance
    if np.max(embedding_l2_distances) / np.min(embedding_l2_distances) > 10:
        fig, ax = plt.subplots(figsize=(16, 8))
        ax.semilogy(singular_vector_indices, embedding_l2_distances, color='purple', linewidth=1, alpha=0.7)
        ax.axhline(y=np.mean(embedding_l2_distances), color='red', linestyle='--',
                   label=f'Mean: {np.mean(embedding_l2_distances):.2e}')
        ax.set_title(f'Embedding L2 Distance at Stability Boundary (Log Scale)\n"{input_text}"',
                     fontsize=14, fontweight='bold')
        ax.set_xlabel('Singular Vector Index')
        ax.set_ylabel('L2 Distance (log scale)')
        ax.grid(True, alpha=0.5, which='both')
        ax.legend()
        plt.tight_layout()
        log_path = os.path.join(output_dir, "embedding_l2_distance_log.png")
        plt.savefig(log_path, dpi=300)
        print(f"Saved log-scale embedding distance plot to: {log_path}")
        plt.close()

    print("\n" + "="*80)
    print("EXPERIMENT COMPLETE!")
    print("="*80)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Experiment 15 Part 2: Singular Vector Stability Boundary L2 Distance Analysis")
    parser.add_argument("npz_file", type=str, help="Path to the .npz file from exp13.")
    parser.add_argument("--output_dir", type=str, default=None,
                        help="Directory to save the results. Defaults to a new dir in the same location as the npz file.")
    parser.add_argument("--precision", type=str, choices=["float32", "float64"], default="float64",
                        help="Precision to use for perturbation calculation.")

    args = parser.parse_args()

    if args.output_dir is None:
        base_dir = os.path.dirname(args.npz_file)
        npz_filename = os.path.splitext(os.path.basename(args.npz_file))[0]
        output_dir = os.path.join(base_dir, f"exp15_part2_analysis_{npz_filename}_{args.precision}")
    else:
        output_dir = args.output_dir

    analyze_singular_vector_distances(args.npz_file, output_dir, args.precision)

'''
python3 exp15_part2_singular_vector_l2_analysis.py "/path/to/exp13/singular_vector_stability_data.npz" --precision "float32"
python3 exp15_part2_singular_vector_l2_analysis.py "/path/to/exp13/singular_vector_stability_data.npz" --output_dir "./my_results"
'''